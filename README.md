# Confidence-Calibration-under-Distribution-Shift
This repository is dedicated to documenting state-of-the-art methods for confidence calibration of classification models under distribution shift, including papers and reproduction code for these methods. The distribution shift include three kinds: Label shift, Covariate shift, and Out-of-distribution.
## Confidence Calibration under Label Shift
| Paper | Source | Year| Code|
|-------|-------|-------|-------|
| [Posterior Re-calibration](https://proceedings.neurips.cc/paper/2020/hash/5ca359ab1e9e3b9c478459944a2d9ca5-Abstract.html) | NeurIPS | 2020 |[https://github.com/GT-RIPL/UNO-IC](https://github.com/GT-RIPL/UNO-IC) |
| [LADE](https://openaccess.thecvf.com/content/CVPR2021/html/Hong_Disentangling_Label_Distribution_for_Long-Tailed_Visual_Recognition_CVPR_2021_paper.html)| CVPR|2021|[https://github.com/hyperconnect/LADE](https://github.com/hyperconnect/LADE)|
| [ADELLO](https://arxiv.org/abs/2306.04621) | ECCV | 2024 |[https://github.com/emasa/ADELLO-LTSSL](https://github.com/emasa/ADELLO-LTSSL) |
## Confidence calibration under Covariate Shift

## Confidence calibration under Out-of-distribution
